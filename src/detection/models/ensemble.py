"""
Ensemble anomaly detection model

Implements ensemble methods for combining multiple anomaly detection models
for improved detection performance

Version: 1.0.0
Last updated: 2025-03-15 18:57:03
Last updated by: Rahul
"""

import numpy as np
import pandas as pd
import pickle
import os
import json
import time
import warnings
from typing import Dict, Any, List, Tuple, Optional, Union, Callable, Set
from pathlib import Path

from src.detection.models.base import BaseModel, ModelConfig
from src.common.logging_config import get_logger

# Initialize logger
logger = get_logger("asira.detection.models.ensemble")

class EnsembleModel(BaseModel):
    """
    Ensemble model for anomaly detection
    
    Combines multiple anomaly detection models for improved performance.
    Supports weighted averaging, majority voting, and maximum score methods.
    """
    
    def __init__(self, config: Union[Dict[str, Any], ModelConfig]):
        """
        Initialize ensemble model
        
        Args:
            config: Model configuration
        """
        if isinstance(config, dict):
            config["model_type"] = "ensemble"
            
        super().__init__(config)
        
        # Extract ensemble parameters
        self.ensemble_method = self.config.config.get("ensemble_method", "average")
        self.model_configs = self.config.config.get("models", [])
        self.model_weights = self.config.config.get("model_weights", {})
        self.dynamic_weights = self.config.config.get("dynamic_weights", False)
        self.batch_size = self.config.config.get("batch_size", 1000)
        
        # Initialize model containers
        self.models: Dict[str, BaseModel] = {}
        self.model_performance: Dict[str, float] = {}
        
        # Import necessary modules only when needed
        self._import_models_lazily()
    
    def _import_models_lazily(self):
        """Import model classes only when needed to avoid circular imports"""
        try:
            # These imports are done inside the function to avoid circular imports
            # They should only be needed at runtime when using the ensemble
            from src.detection.models.statistical import StatisticalModel
            from src.detection.models.isolation_forest import IsolationForestModel
            from src.detection.models.autoencoder import AutoencoderModel
            
            self._model_classes = {
                "statistical": StatisticalModel,
                "isolation_forest": IsolationForestModel,
                "autoencoder": AutoencoderModel,
                # Add other model types here as they're implemented
            }
        except ImportError as e:
            logger.warning(f"Error importing model classes: {e}")
            self._model_classes = {}
    
    def add_model(self, model: BaseModel, model_id: Optional[str] = None, weight: float = 1.0) -> str:
        """
        Add a model to the ensemble
        
        Args:
            model: Model to add
            model_id: Optional ID for the model (autogenerated if None)
            weight: Weight for this model in the ensemble
            
        Returns:
            Model ID
        """
        if model_id is None:
            # Generate a unique ID based on model type and timestamp
            model_id = f"{model.config.model_type}_{int(time.time())}"
            
        # Make sure ID is unique
        if model_id in self.models:
            model_id = f"{model_id}_{len(self.models)}"
            
        self.models[model_id] = model
        self.model_weights[model_id] = weight
        
        logger.info(f"Added model {model_id} to ensemble with weight {weight}")
        
        return model_id
    
    def remove_model(self, model_id: str) -> bool:
        """
        Remove a model from the ensemble
        
        Args:
            model_id: ID of model to remove
            
        Returns:
            True if model was removed, False if not found
        """
        if model_id in self.models:
            del self.models[model_id]
            if model_id in self.model_weights:
                del self.model_weights[model_id]
            if model_id in self.model_performance:
                del self.model_performance[model_id]
                
            logger.info(f"Removed model {model_id} from ensemble")
            return True
        else:
            logger.warning(f"Model {model_id} not found in ensemble")
            return False
    
    def set_model_weight(self, model_id: str, weight: float) -> bool:
        """
        Set the weight of a model in the ensemble
        
        Args:
            model_id: Model ID
            weight: New weight
            
        Returns:
            True if weight was set, False if model not found
        """
        if model_id in self.models:
            self.model_weights[model_id] = weight
            logger.info(f"Set weight of model {model_id} to {weight}")
            return True
        else:
            logger.warning(f"Model {model_id} not found in ensemble")
            return False
    
    def _initialize_models(self) -> None:
        """
        Initialize models based on configuration
        """
        if not self.model_configs and not self.models:
            raise ValueError("No models specified in configuration")
            
        # If models are already initialized, skip
        if self.models:
            return
            
        # Make sure model classes are imported
        if not hasattr(self, "_model_classes"):
            self._import_models_lazily()
            
        # Initialize models from configs
        for i, model_config in enumerate(self.model_configs):
            try:
                # Extract model type and ID
                model_type = model_config.get("model_type")
                model_id = model_config.get("model_id", f"{model_type}_{i}")
                
                if model_type not in self._model_classes:
                    logger.warning(f"Unknown model type: {model_type}")
                    continue
                    
                # Create model
                model_class = self._model_classes[model_type]
                model = model_class(model_config)
                
                # Add to ensemble
                weight = self.model_weights.get(model_id, 1.0)
                self.add_model(model, model_id, weight)
                
            except Exception as e:
                logger.error(f"Error initializing model: {e}")
    
    def train(self, X: np.ndarray, y: Optional[np.ndarray] = None) -> None:
        """
        Train all models in the ensemble
        
        Args:
            X: Training data
            y: Optional training data labels (for supervised models)
        """
        if X.shape[0] == 0:
            raise ValueError("Cannot train on empty dataset")
            
        start_time = time.time()
        
        # Initialize models if needed
        self._initialize_models()
        
        if not self.models:
            raise ValueError("No models in ensemble")
        
        # Train each model
        for model_id, model in self.models.items():
            logger.info(f"Training model {model_id}")
            try:
                model.train(X, y)
            except Exception as e:
                logger.error(f"Error training model {model_id}: {e}")
        
        # If dynamic weights are enabled and labeled data is available,
        # adjust weights based on performance
        if self.dynamic_weights and y is not None:
            self._adjust_weights_by_performance(X, y)
            
        # Record training stats
        self.training_stats = {
            "n_samples": X.shape[0],
            "n_features": X.shape[1],
            "n_models": len(self.models),
            "model_ids": list(self.models.keys()),
            "model_weights": {k: float(v) for k, v in self.model_weights.items()},
            "training_time": time.time() - start_time,
            "ensemble_method": self.ensemble_method
        }
        
        self.trained = True
        logger.info(f"Ensemble model trained with {len(self.models)} models")
    
    def _adjust_weights_by_performance(self, X: np.ndarray, y: np.ndarray) -> None:
        """
        Adjust model weights based on performance
        
        Args:
            X: Validation data
            y: True labels (1 for anomaly, 0 for normal)
        """
        logger.info("Adjusting model weights based on performance")
        
        # Get performance for each model
        for model_id, model in self.models.items():
            try:
                # Get predictions
                scores = model.predict(X)
                
                # Apply threshold to get binary predictions
                threshold = model.threshold
                preds = (scores >= threshold).astype(int)
                
                # Calculate F1 score
                tp = np.sum((preds == 1) & (y == 1))
                fp = np.sum((preds == 1) & (y == 0))
                fn = np.sum((preds == 0) & (y == 1))
                
                precision = tp / (tp + fp) if (tp + fp) > 0 else 0
                recall = tp / (tp + fn) if (tp + fn) > 0 else 0
                f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0
                
                # Store performance
                self.model_performance[model_id] = f1
                
            except Exception as e:
                logger.error(f"Error calculating performance for model {model_id}: {e}")
                self.model_performance[model_id] = 0.0
        
        # Normalize performance to get weights (add small epsilon to avoid division by zero)
        total_performance = sum(self.model_performance.values()) + 1e-10
        
        for model_id in self.models:
            if model_id in self.model_performance:
                self.model_weights[model_id] = self.model_performance[model_id] / total_performance
                
        logger.info(f"Adjusted weights: {self.model_weights}")
    
    def predict(self, X: np.ndarray) -> np.ndarray:
        """
        Generate ensemble predictions
        
        Args:
            X: Data to predict
            
        Returns:
            Array of anomaly scores
        """
        if not self.trained:
            raise ValueError("Model not trained. Call train() first.")
        
        if not self.models:
            raise ValueError("No models in ensemble")
            
        # For large datasets, process in batches
        if X.shape[0] > self.batch_size:
            return self._predict_batched(X)
        
        # Get predictions from each model
        all_predictions = {}
        for model_id, model in self.models.items():
            try:
                scores = model.predict(X)
                all_predictions[model_id] = scores
            except Exception as e:
                logger.error(f"Error getting predictions from model {model_id}: {e}")
                
        # Combine predictions
        if not all_predictions:
            raise ValueError("No valid predictions from ensemble models")
            
        return self._combine_predictions(all_predictions)
    
    def _predict_batched(self, X: np.ndarray) -> np.ndarray:
        """
        Generate predictions in batches
        
        Args:
            X: Data to predict
            
        Returns:
            Array of anomaly scores
        """
        results = []
        
        for i in range(0, X.shape[0], self.batch_size):
            end_idx = min(i + self.batch_size, X.shape[0])
            batch = X[i:end_idx]
            batch_scores = self.predict(batch)
            results.append(batch_scores)
            
        return np.concatenate(results)
    
    def _combine_predictions(self, predictions: Dict[str, np.ndarray]) -> np.ndarray:
        """
        Combine predictions from multiple models
        
        Args:
            predictions: Dictionary mapping model IDs to predictions
            
        Returns:
            Combined predictions
        """
        if self.ensemble_method == "average" or self.ensemble_method == "weighted_average":
            # Weighted average
            combined = np.zeros(list(predictions.values())[0].shape)
            total_weight = 0.0
            
            for model_id, scores in predictions.items():
                weight = self.model_weights.get(model_id, 1.0)
                combined += scores * weight
                total_weight += weight
                
            if total_weight > 0:
                combined /= total_weight
            
            return combined
            
        elif self.ensemble_method == "maximum":
            # Take maximum score across models
            all_scores = np.stack(list(predictions.values()), axis=1)
            return np.max(all_scores, axis=1)
            
        elif self.ensemble_method == "minimum":
            # Take minimum score across models
            all_scores = np.stack(list(predictions.values()), axis=1)
            return np.min(all_scores, axis=1)
            
        elif self.ensemble_method == "median":
            # Take median score across models
            all_scores = np.stack(list(predictions.values()), axis=1)
            return np.median(all_scores, axis=1)
            
        elif self.ensemble_method == "majority_vote":
            # Convert scores to binary decisions using each model's threshold
            binary_decisions = {}
            
            for model_id, model in self.models.items():
                if model_id in predictions:
                    threshold = model.threshold
                    binary_decisions[model_id] = (predictions[model_id] >= threshold).astype(int)
            
            # Stack decisions and take majority vote
            decisions = np.stack(list(binary_decisions.values()), axis=1)
            vote_counts = np.sum(decisions, axis=1)
            
            # Convert back to [0,1] scores
            return vote_counts / decisions.shape[1]
        
        else:
            logger.warning(f"Unknown ensemble method: {self.ensemble_method}. Using average.")
            # Default to simple average
            all_scores = np.stack(list(predictions.values()), axis=1)
            return np.mean(all_scores, axis=1)
    
    def predict_with_explanation(self, X: np.ndarray) -> Tuple[np.ndarray, List[Dict[str, float]]]:
        """
        Predict with explanations from ensemble
        
        Args:
            X: Data to predict
            
        Returns:
            Tuple of (scores, explanations)
        """
        if not self.trained:
            raise ValueError("Model not trained. Call train() first.")
            
        # Get anomaly scores
        anomaly_scores = self.predict(X)
        
        # Get explanations from each model
        all_explanations = {}
        model_explanations = {}
        
        for model_id, model in self.models.items():
            try:
                scores, explanations = model.predict_with_explanation(X)
                all_explanations[model_id] = explanations
                model_explanations[model_id] = scores
            except Exception as e:
                logger.error(f"Error getting explanations from model {model_id}: {e}")
        
        # Combine explanations
        combined_explanations = []
        
        if not all_explanations:
            # If no explanations, create empty ones
            for i in range(X.shape[0]):
                feature_scores = {}
                for j in range(X.shape[1]):
                    feature_name = self.feature_names[j] if j < len(self.feature_names) else f"feature_{j}"
                    feature_scores[feature_name] = 1.0 / X.shape[1]  # Uniform importance
                combined_explanations.append(feature_scores)
                
        else:
            # For each sample
            for i in range(X.shape[0]):
                # Initialize combined feature scores
                combined_feature_scores = {}
                feature_count = {}
                
                # For each model
                for model_id, explanations in all_explanations.items():
                    # Skip if model has no explanation for this sample
                    if i >= len(explanations):
                        continue
                        
                    model_weight = self.model_weights.get(model_id, 1.0)
                    explanation = explanations[i]
                    
                    # For each feature in the explanation
                    for feature, score in explanation.items():
                        if feature not in combined_feature_scores:
                            combined_feature_scores[feature] = 0
                            feature_count[feature] = 0
                            
                        # Add weighted score
                        combined_feature_scores[feature] += score * model_weight
                        feature_count[feature] += 1
                
                # Normalize scores
                for feature in combined_feature_scores:
                    if feature_count[feature] > 0:
                        combined_feature_scores[feature] /= feature_count[feature]
                        
                # Normalize to sum to 1
                total = sum(combined_feature_scores.values()) or 1.0
                for feature in combined_feature_scores:
                    combined_feature_scores[feature] /= total
                    
                combined_explanations.append(combined_feature_scores)
        
        return anomaly_scores, combined_explanations
    
    def get_model_contributions(self, X: np.ndarray) -> Tuple[np.ndarray, Dict[str, np.ndarray]]:
        """
        Get the contribution of each model to the final scores
        
        Args:
            X: Data to predict
            
        Returns:
            Tuple of (combined_scores, model_scores_dict)
        """
        if not self.trained:
            raise ValueError("Model not trained. Call train() first.")
            
        # Get predictions from each model
        model_scores = {}
        for model_id, model in self.models.items():
            try:
                scores = model.predict(X)
                model_scores[model_id] = scores
            except Exception as e:
                logger.error(f"Error getting predictions from model {model_id}: {e}")
        
        # Get combined scores
        combined_scores = self._combine_predictions(model_scores)
        
        return combined_scores, model_scores
    
    def analyze_ensemble_disagreement(self, X: np.ndarray) -> Dict[str, Any]:
        """
        Analyze disagreement between models in the ensemble
        
        Args:
            X: Data to analyze
            
        Returns:
            Dictionary with disagreement statistics
        """
        if not self.trained:
            raise ValueError("Model not trained. Call train() first.")
            
        # Get predictions from each model
        _, model_scores = self.get_model_contributions(X)
        
        # Calculate pairwise correlations
        model_ids = list(model_scores.keys())
        correlations = {}
        
        for i, model_id1 in enumerate(model_ids):
            for j, model_id2 in enumerate(model_ids):
                if i >= j:
                    continue
                    
                scores1 = model_scores[model_id1]
                scores2 = model_scores[model_id2]
                
                # Calculate correlation
                correlation = np.corrcoef(scores1, scores2)[0, 1]
                correlations[f"{model_id1}_vs_{model_id2}"] = float(correlation)
        
        # Calculate disagreement score (based on standard deviation of scores)
        score_matrix = np.stack([model_scores[model_id] for model_id in model_ids], axis=1)
        disagreement = np.std(score_matrix, axis=1)
        
        # Calculate agreement on top anomalies
        top_anomalies = {}
        n_top = min(10, X.shape[0])
        
        for model_id, scores in model_scores.items():
            top_indices = np.argsort(scores)[-n_top:][::-1]
            top_anomalies[model_id] = set(top_indices)
        
        # Calculate Jaccard similarity between top anomalies
        jaccard_similarity = {}
        
        for i, model_id1 in enumerate(model_ids):
            for j, model_id2 in enumerate(model_ids):
                if i >= j:
                    continue
                    
                set1 = top_anomalies[model_id1]
                set2 = top_anomalies[model_id2]
                
                # Calculate Jaccard similarity: |A ∩ B| / |A ∪ B|
                intersection = len(set1.intersection(set2))
                union = len(set1.union(set2))
                
                similarity = intersection / union if union > 0 else 0
                jaccard_similarity[f"{model_id1}_vs_{model_id2}"] = float(similarity)
        
        return {
            "correlations": correlations,
            "mean_correlation": float(np.mean(list(correlations.values()))),
            "disagreement_per_sample": disagreement.tolist(),
            "mean_disagreement": float(np.mean(disagreement)),
            "jaccard_similarity": jaccard_similarity,
            "mean_jaccard": float(np.mean(list(jaccard_similarity.values())))
        }
    
    def _save_model_data(self, path: str) -> str:
        """
        Save model data to disk
        
        Args:
            path: Directory path
            
        Returns:
            Path to saved models directory
        """
        # Create directory for ensemble models
        models_dir = os.path.join(path, "ensemble_models")
        os.makedirs(models_dir, exist_ok=True)
        
        # Save each model
        saved_models = {}
        
        for model_id, model in self.models.items():
            try:
                # Create directory for this model
                model_dir = os.path.join(models_dir, model_id)
                os.makedirs(model_dir, exist_ok=True)
                
                # Save model
                model_path = model.save(model_dir)
                saved_models[model_id] = {
                    "path": model_path,
                    "type": model.config.model_type
                }
                
            except Exception as e:
                logger.error(f"Error saving model {model_id}: {e}")
        
        # Save ensemble metadata
        ensemble_data = {
            "ensemble_method": self.ensemble_method,
            "model_weights": {k: float(v) for k, v in self.model_weights.items()},
            "model_performance": {k: float(v) for k, v in self.model_performance.items()},
            "saved_models": saved_models,
            "dynamic_weights": self.dynamic_weights,
        }
        
        metadata_path = os.path.join(path, "ensemble_metadata.json")
        with open(metadata_path, "w") as f:
            json.dump(ensemble_data, f, indent=2)
            
        return models_dir
        
    @classmethod
    def load(cls, path: str) -> 'EnsembleModel':
        """
        Load model from disk
        
        Args:
            path: Path to model directory
            
        Returns:
            Loaded model
        """
        # Load configuration
        config_path = os.path.join(path, "config.json")
        with open(config_path, "r") as f:
            config = ModelConfig.from_json(f.read())
            
        # Create model instance
        model = cls(config)
        
        # Load metadata
        metadata_path = os.path.join(path, "metadata.json")
        with open(metadata_path, "r") as f:
            metadata = json.load(f)
            
        model.trained = metadata["trained"]
        model.threshold = metadata["threshold"]
        model.training_stats = metadata["training_stats"]
        model.feature_names = metadata["feature_names"]
        
        # Load ensemble metadata
        ensemble_metadata_path = os.path.join(path, "ensemble_metadata.json")
        if os.path.exists(ensemble_metadata_path):
            with open(ensemble_metadata_path, "r") as f:
                ensemble_data = json.load(f)
                
            model.ensemble_method = ensemble_data.get("ensemble_method", model.ensemble_method)
            model.model_weights = ensemble_data.get("model_weights", {})
            model.model_performance = ensemble_data.get("model_performance", {})
            model.dynamic_weights = ensemble_data.get("dynamic_weights", False)
            
            # Make sure model classes are imported
            model._import_models_lazily()
            
            # Load each model
            models_dir = os.path.join(path, "ensemble_models")
            if os.path.exists(models_dir):
                saved_models = ensemble_data.get("saved_models", {})
                
                for model_id, model_info in saved_models.items():
                    try:
                        model_type = model_info.get("type")
                        model_dir = os.path.join(models_dir, model_id)
                        
                        if model_type not in model._model_classes:
                            logger.warning(f"Unknown model type: {model_type}")
                            continue
                            
                        # Load the model
                        model_class = model._model_classes[model_type]
                        loaded_model = model_class.load(model_dir)
                        
                        # Add to ensemble with original weight
                        weight = model.model_weights.get(model_id, 1.0)
                        model.add_model(loaded_model, model_id, weight)
                        
                    except Exception as e:
                        logger.error(f"Error loading model {model_id}: {e}")
        
        # Load preprocessors if they exist
        preprocessors_path = os.path.join(path, "preprocessors.pkl")
        if os.path.exists(preprocessors_path):
            with open(preprocessors_path, "rb") as f:
                model._preprocessors = pickle.load(f)
        
        return model
    
    def get_feature_importance_summary(self, X: np.ndarray) -> Dict[str, float]:
        """
        Get overall feature importance across the ensemble
        
        Args:
            X: Data to analyze
            
        Returns:
            Dictionary mapping feature names to importance scores
        """
        if not self.trained:
            raise ValueError("Model not trained. Call train() first.")
            
        # Get explanations
        _, explanations = self.predict_with_explanation(X)
        
        # Aggregate feature importance across all samples
        feature_importance = {}
        
        for explanation in explanations:
            for feature, importance in explanation.items():
                if feature not in feature_importance:
                    feature_importance[feature] = 0
                feature_importance[feature] += importance
                
        # Normalize
        total = sum(feature_importance.values()) or 1.0
        for feature in feature_importance:
            feature_importance[feature] /= total
            
        return feature_importance
    
    def visualize_model_comparison(self, X: np.ndarray, top_n: int = 20) -> Any:
        """
        Visualize and compare model predictions
        
        Args:
            X: Data to analyze
            top_n: Number of top anomalies to include
            
        Returns:
            Plot object
        """
        try:
            import matplotlib.pyplot as plt
            
            # Get predictions from each model
            ensemble_scores, model_scores = self.get_model_contributions(X)
            
            # Get top anomalies from ensemble
            top_indices = np.argsort(ensemble_scores)[-top_n:][::-1]
            
            # Extract scores for top anomalies
            model_names = list(model_scores.keys())
            comparison_data = []
            
            for idx in top_indices:
                row = {"sample_idx": idx, "ensemble": ensemble_scores[idx]}
                for model_id in model_names:
                    row[model_id] = model_scores[model_id][idx]
                comparison_data.append(row)
                
            # Create plot
            plt.figure(figsize=(12, 8))
            bar_width = 0.8 / (len(model_names) + 1)  # +1 for ensemble
            
            # Plot ensemble scores
            positions = np.arange(len(comparison_data))
            plt.bar(
                positions - bar_width * (len(model_names) / 2),
                [d["ensemble"] for d in comparison_data],
                bar_width,
                label="Ensemble",
                alpha=0.7,
                color='black'
            )
            
            # Plot individual model scores
            for i, model_id in enumerate(model_names):
                plt.bar(
                    positions - bar_width * (len(model_names) / 2) + bar_width * (i + 1),
                    [d[model_id] for d in comparison_data],
                    bar_width,
                    label=model_id,
                    alpha=0.7
                )
            
            plt.xlabel("Sample Index")
            plt.ylabel("Anomaly Score")
            plt.title("Top Anomalies: Model Comparison")
            plt.xticks(positions, [d["sample_idx"] for d in comparison_data], rotation=90)
            plt.legend()
            plt.tight_layout()
            
            return plt
            
        except ImportError:
            logger.warning("Matplotlib not available. Cannot create visualization.")
            return None


def create_ensemble_from_models(models: Dict[str, BaseModel], 
                               ensemble_method: str = "average", 
                               model_weights: Optional[Dict[str, float]] = None) -> EnsembleModel:
    """
    Create an ensemble from existing models
    
    Args:
        models: Dictionary mapping model IDs to model instances
        ensemble_method: Method for combining predictions
        model_weights: Optional dictionary mapping model IDs to weights
        
    Returns:
        EnsembleModel instance
    """
    # Create basic config
    config = {
        "model_type": "ensemble",
        "ensemble_method": ensemble_method
    }
    
    # Create ensemble
    ensemble = EnsembleModel(config)
    
    # Add models
    for model_id, model in models.items():
        weight = model_weights.get(model_id, 1.0) if model_weights else 1.0
        ensemble.add_model(model, model_id, weight)
        
    return ensemble


def optimize_ensemble_weights(ensemble: EnsembleModel, X: np.ndarray, y: np.ndarray) -> Dict[str, float]:
    """
    Optimize weights for ensemble models using labeled data
    
    Args:
        ensemble: Ensemble model
        X: Validation data
        y: True labels (1 for anomaly, 0 for normal)
        
    Returns:
        Dictionary of optimized weights
    """
    try:
        from scipy.optimize import minimize
        
        # Get scores from each model
        _, model_scores = ensemble.get_model_contributions(X)
        model_ids = list(model_scores.keys())
        
        # Objective function to minimize (negative F1 score)
        def objective(weights):
            # Normalize weights
            weights = np.abs(weights)  # Ensure weights are positive
            if np.sum(weights) > 0:
                weights = weights / np.sum(weights)
                
            # Combine scores with these weights
            combined_scores = np.zeros(X.shape[0])
            for i, model_id in enumerate(model_ids):
                combined_scores += weights[i] * model_scores[model_id]
                
            # Apply threshold to get binary predictions
            threshold = ensemble.threshold
            preds = (combined_scores >= threshold).astype(int)
            
            # Calculate F1 score
            tp = np.sum((preds == 1) & (y == 1))
            fp = np.sum((preds == 1) & (y == 0))
            fn = np.sum((preds == 0) & (y == 1))
            
            precision = tp / (tp + fp) if (tp + fp) > 0 else 0
            recall = tp / (tp + fn) if (tp + fn) > 0 else 0
            f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0
            
            # Return negative F1 (since we want to maximize F1, but minimize objective)
            return -f1
        
        # Initial weights (equal weighting)
        initial_weights = np.ones(len(model_ids)) / len(model_ids)
        
        # Constraints: weights sum to 1, weights >= 0
        constraints = [{'type': 'eq', 'fun': lambda w: np.sum(w) - 1}]
        bounds = [(0, 1) for _ in model_ids]
        
        # Run optimization
        result = minimize(
            objective, 
            initial_weights, 
            method='SLSQP',
            bounds=bounds,
            constraints=constraints
        )
        
        # Get optimized weights
        optimized_weights = result.x
        
        # Normalize again to ensure sum = 1
        optimized_weights = optimized_weights / np.sum(optimized_weights)
        
        # Update ensemble weights
        optimized_weight_dict = {model_id: float(weight) for model_id, weight in zip(model_ids, optimized_weights)}
        
        for model_id, weight in optimized_weight_dict.items():
            ensemble.set_model_weight(model_id, weight)
            
        logger.info(f"Optimized weights: {optimized_weight_dict}")
        
        return optimized_weight_dict
        
    except ImportError:
        logger.warning("scipy not available. Using uniform weighting.")
        model_ids = list(ensemble.models.keys())
        weight = 1.0 / len(model_ids)
        return {model_id: weight for model_id in model_ids}


# Module version information
__version__ = "1.0.0"
__last_updated__ = "2025-03-15 18:59:46"
__author__ = "Rahul"
